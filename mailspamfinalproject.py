# -*- coding: utf-8 -*-
"""MailSpamFinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SNDLLXx7otu-4CCU8Dx59rHifAjmQpzV
"""

from google.colab import files
sms_spam_data = files.upload()

import io
df = pd.read_csv(io.StringIO(sms_spam_data['spam.csv'].decode('ISO-8859-1')))
df

import pandas as pd
import numpy as np
import matplotlib as plt
import matplotlib.pyplot as plt
import nltk
from wordcloud import WordCloud

#puncuation

nltk.download('punkt')

nltk.download('stopwords')

data=pd.read_csv('spam.csv',encoding='latin-1')
data.head()

"""# cleaning the data such as remove unused data """

data=data.drop(["Unnamed: 2","Unnamed: 3","Unnamed: 4"],axis=1)

data = data.rename(columns={"v2" : "text" , "v1":"label"})

data['label'].value_counts()

# Visualization

data['length']=data['text'].map(lambda x:len(x))
data.hist(column='length' ,bins=50,figsize=(10,7),color='green')

data.hist(column="length" , by='label',bins=100,figsize=(20,7))

# find ham and spam frequent words

ham_words=" "
spam_words=" "

for val in data[data['label'] == 'spam'].text:
  text=val.lower()
  tokens=nltk.word_tokenize(text)
  for words in tokens:
    spam_words=spam_words + words +' '

for val in data[data['label'] == 'ham'].text:
  text=val.lower()
  tokens=nltk.word_tokenize(text)
  for words in tokens:
    ham_words=ham_words + words +' '

spam_wordcloud=WordCloud(width=500,height=100).generate(spam_words)
ham_wordcloud=WordCloud(width=300,height=300).generate(ham_words)

plt.figure( figsize=(10,8), facecolor='w')
plt.imshow(spam_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

plt.figure( figsize=(10,8), facecolor='w')
plt.imshow(ham_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

data=data.replace(['ham','spam'],[0,1])
data.head(10)

# converting words to vectors

from sklearn.model_selection import train_test_split

data, labels = np.arange(10).reshape((5, 2)), range(5)

data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.20, random_state=42)

!pip install matplotlib-venn

!apt-get -qq install -y libfluidsynth1

# https://pypi.python.org/pypi/libarchive
!apt-get -qq install -y libarchive-dev && pip install -U libarchive
import libarchive

# https://pypi.python.org/pypi/pydot
!apt-get -qq install -y graphviz && pip install pydot
import pydot

!pip install cartopy
import cartopy

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer =  TfidfVectorizer()
vectors  = vectorizer.fit_transform(data['text'])
vectors.shape

# word vector

feature=vectors

# splitting into training and testing data set

from sklearn.model_selection import train_test_split
X_train, X_test, y_train,y_test = train_test_split(feature,data['label'],test_size = 0.15, random_state=111)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)



#add upward sampling

from collections import Counter
from imblearn import under_sampling, over_sampling
from imblearn.over_sampling import SMOTE

smote=SMOTE(sampling_strategy={1:4000},random_state=42)
X_train,y_train=smote.fit_resample(X_train.astype('float'),y_train)
print("Before SMOTE", Counter(y_train))

# Training the Model using Vectors without TF-IDF

import xgboost as xgb
d_train=xgb.DMatrix(X_train,label=y_train)
param={}
watchlist=[(d_train,'train')]
bst = xgb.train(param,d_train ,400,watchlist,early_stopping_rounds=50,verbose_eval=10)

# Evaluation of model using TF-IDF

from sklearn.metrics import accuracy_score
# predict value for test case
d_test=xgb.DMatrix(X_test)
p_test=bst.predict(d_test)
# apply round function to each element of np array so predict either 0 or 1
npround=np.vectorize(round)
p_test_ints=npround(p_test)
# error rate
accuracy=accuracy_score(y_test,p_test_ints)
print(accuracy)

# classification report

from sklearn.metrics import classification_report
print(classification_report(y_test,p_test_ints))

# classifying using sklearn pre built classifier

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

svc=SVC(kernel='sigmoid',gamma=1.0)
knc=KNeighborsClassifier(n_neighbors=49)
mnb=MultinomialNB(alpha=0.2)

lrc=LogisticRegression(solver='liblinear',penalty='l1')
rfc=RandomForestClassifier(n_estimators=31,random_state=111)

clfs={'SVC' : svc,'KN' : knc,'NB' : mnb,'LR' : lrc,'RF': rfc}

def train(clf,features,targets):
  clf.fit(features,targets)

def predict(clf,features):
  return(clf.predict(features))

pred_scores_word_vectors=[]
for k,v in clfs.items():
  train(v,X_train,y_train)
  pred=predict(v,X_test)
  pred_scores_word_vectors.append((k,[accuracy_score(y_test,pred)]))

# checking prediction for each and every algorithm

predictions=pd.DataFrame.from_dict(dict(pred_scores_word_vectors),orient='index',columns=['Score'])
predictions

def find(p):
 if p==1:
   print("message is span")
 else:
  print("message is not span")

text=["hello how are you...........free gifts"]
integers=vectorizer.transform(text)

p= mnb.predict(integers)[0]
find(p)

# pickle file-storinf machine learning model pattern

import  joblib
joblib.dump(mnb,'best.pkl')
print("Best Model(Naive Bayes) Saved")

best=joblib.load('best.pkl')
best